# 4052project


The objective of this project is to develop a multimodal emotion recognition system that facilitates human-computer interaction. It will take audio/images as input and try to predict the emotional state of the input. 

Vision: processes images and tries to classify them. Images will be processeds as tensors, where each frame is represented by a matrix of pixel intensity.

NLP: processes audio and tries to classify it. I will attempt to use audio spectrograms.

Datasets: FER-2013 for labeled facial expressions, CMU-MOSEI for labeled visual and audio recordings.
